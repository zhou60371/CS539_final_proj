<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link rel="stylesheet" href="css/index.css">
</head>

<body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <div class="container-fluid">
            <a class="navbar-brand" href="#">CS539 Final Project</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false"
                aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                    <li class="nav-item">
                        <a class="nav-link active" id="overview" href="#">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" id="dateset" href="#">Dateset</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" id="model" href="#">Model</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" id="result" href="#">Result</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" id="demo" href="#">Demo</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" id="team" href="#">Team</a>
                    </li>
                    <li class="nav-item">
                        <a style="padding-left: 20px;" class="nav-link"
                            href="https://github.com/MatthewKKai/CS539_final_proj"><svg
                                xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
                                class="bi bi-github" viewBox="0 0 16 16">
                                <path
                                    d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" />
                            </svg></a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <div class="container col-md-8 offser-md-2">
        <div class="overview">
            <p class="h3" style="padding-top: 30px;">Background</p>
            <div>
                With the rapid development of technology, a large number of multi-modality data have been created
                through internet and how to process these multi-modality data has
                become a hotspot topic. Image-text matching task is for
                this topic, which has drawn plenty attentions in computer
                vision (CV) and natural language processing (NLP) fields. Traditionally, this problem has been explored
                by
                directly comparing the similarity between semantic space
                features from both text and image data. However, this
                method is inherently to be computationally intensive as it
                requires fined understanding of both image and text data.
            </div>
            <p class="h3" style="padding-top: 30px;">Overview</p>
            <div>
                In this work, we explore the image-text retrival task with an interest of coarse and fine representation
                feature learning. We propose a two branch network that extracts features with different level of details
                from text and image data. Specifically, we evaluated the combination of image features from ResNet, VGG
                and text features from word2vec, Bert as coarse and fine feature respectively. Our two branch network
                forms an siamese network strcture. By optimizing the contrastive loss during trianing, we achieve binary
                image-text matching accuracy as high as 71.1% in our dataset. Our results also show that classification
                accuracy does not linearly scale up with model complexity, which motivates future optmization for more
                efficient model design and representation learning.
            </div>
            <!-- <div>
                <img src="images/figure1.png" width="80%" class="rounded mx-auto d-block" alt="">
            </div> -->

        </div>
        <div class="dataset">
            <p class="h3" style="padding-top: 30px;">Data source</p>
            <div>
                In this work, we will use <a href="https://cocodataset.org/#home">Microsoft Common Objects in
                    Context (COCO)</a> as the dataset of our task. COCO consists
                of 123287 images, and each one is associated with five sen-
                tences. Due to the limit computing resources, we will use
                part of them.
            </div>
            <div>
                <img src="images/figure2.png" style="padding-top: 30px;" width="80%" class="rounded mx-auto d-block"
                    alt="">
            </div>
            <div style="padding-top: 30px;">We use <a
                    href="https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb">COCO API</a> to
                mapulate data source. The COCO API assists in loading,
                parsing, and visualizing
                annotations in COCO. The API supports multiple annotation formats./div>
            </div>
        </div>
        <div class="model">
            <p class="h3" style="padding-top: 30px;">PipeLine</p>
            <div class="text-center">
                <img src="images/figure3.png" style="padding-top: 30px;" width="80%" class="rounded mx-auto d-block"
                    alt="">
            </div>
            <p class="h3" style="padding-top: 30px;">Image Processor</p>
            <div class="card text-center">
                <div class="card-header">
                    ResNet
                </div>
                <div class="card-body">
                    <img src="images/figure4.png" style="padding-top: 30px;" width="80%" class="rounded mx-auto d-block"
                        alt="">
                </div>
            </div>
            <div class="card text-center">
                <div class="card-header">
                    VGG19
                </div>
                <div class="card-body">
                    <img src="images/figure5.png" style="padding-top: 30px;" width="80%" class="rounded mx-auto d-block"
                        alt="">
                </div>
            </div>
            <p class="h3" style="padding-top: 30px;">Text Procesoor</p>
            <div class="card text-center">
                <div class="card-header">
                    Tokenizer + Bert
                </div>
                <div class="card-body">
                    <img src="images/figure6.png" style="padding-top: 30px;" width="80%" class="rounded mx-auto d-block"
                        alt="">
                </div>
            </div>
            <div class="card text-center">
                <div class="card-header">
                    Word2Vec: Skip-Gram,Â CBOW
                </div>
                <div class="card-body">
                    <img src="images/figure7.png" style="padding-top: 30px;" width="80%" class="rounded mx-auto d-block"
                        alt="">
                </div>
            </div>
            <p class="h3" style="padding-top: 30px;">References</p>
            <ul>
                <li>[1] Chunxiao Liu, Zhedong Mao, Tianzhu Zhang, Hongtao Xie, Bin Wang and Yongdong Zhang. Graph
                    structured
                    network for image-text matching. arXiv: 2004.00277v1, 2020.
                </li>
                <li>[2] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, Mingliang Xu and Yi-Dong Shen. Dual-path
                    convolutional image-text embedding with instance loss. arXiv: 1711.05535v4, 2021.</li>
            </ul>
        </div>
        <div class="result">
            <p class="h3" style="padding-top: 30px;">Accuracy</p>
            <div class="text-center">
                <img src="images/figure8.png" style="padding-top: 30px;" width="80%" class="rounded mx-auto d-block"
                    alt="">
            </div>
            <p class="h3" style="padding-top: 30px;">Conclusion</p>
            <div>
                <ul style="padding-top: 20px;">
                    <li>
                        <div>
                            We evaluated the text-image retrieval performance over the combination of several common
                            visual and
                            language models.
                        </div>
                    </li>
                    <li style="padding-top: 10px;">
                        <div>
                            We show that although deeper models (Bert, VGG19) achieve better accuracy, they tend to use
                            significant
                            more computation resources.
                        </div>
                    </li>
                </ul>
            </div>
            <p class="h3" style="padding-top: 30px;">Future Work</p>
            <div>
                Current finding suggests relatively low information density in learnt features. Thus, its worth to
                explore more resource efficient learning models and feature representations.
            </div>
        </div>
        <div class="demo">
            <div style="padding-top: 80px;">
                <video width="100%" src="video/video2.mp4" controls></video>
            </div>

        </div>
        <div class="team">
            <p class="h3" style="padding-top: 30px;">Menbers</p>
            <div>
                <span class="fw-bold">Yiqin Zhao</span><span>
                    <a href="mailto: yzhao11@wpi.edu">yzhao11@wpi.edu</a>
                </span>
            </div>
            <div>
                <span class="fw-bold">Kai Zhang</span><span> <a
                        href="mailto: kzhang8@wpi.edu">kzhang8@wpi.edu</a></span>
            </div>
            <div>
                <span class="fw-bold">Yang Wu</span><span> <a href="mailto: ywu19@wpi.edu">ywu19@wpi.edu</a></span>
            </div>
            <div>
                <span class="fw-bold">Zihao Zhou</span><span> <a href="mailto: zzhou5@wpi.edu">zzhou5@wpi.edu</a></span>
            </div>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>
    <script src="js/index.js"></script>
</body>

</html>